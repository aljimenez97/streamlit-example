{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv files\n",
    "transactions = pd.read_csv(\"data/transactions_train.csv\",parse_dates = ['t_dat'])\n",
    "articles = pd.read_csv(\"data/articles.csv\")\n",
    "customers = pd.read_csv(\"data/customers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of articles, number of customers and total volume change over the time\n",
    "\n",
    "# aggregate the transaction by date\n",
    "transaction_aggr = transactions.groupby(['t_dat']).nunique().reset_index()[['t_dat','customer_id','article_id']]\n",
    "\n",
    "# create a column showing sum per user per day\n",
    "transaction_aggr['sum'] = transactions.groupby(['t_dat']).sum().reset_index()[['price']]\n",
    "transaction_aggr = transaction_aggr.rename(columns={\"customer_id\": \"nr_customer\",\n",
    "                                                   \"article_id\":\"nr_article\",\n",
    "                                                   \"sum\":\"total_volume\"})\n",
    "\n",
    "# Save csv\n",
    "transaction_aggr.to_csv(\"data/transaction_aggr.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors\n",
    "\n",
    "# join transaction dataset with articles\n",
    "transaction_article_colour = pd.merge(transactions[['article_id', 't_dat']], articles[['article_id', 'colour_group_name']], how = 'inner', on = ['article_id']).reset_index()\n",
    "\n",
    "# colours articles by month\n",
    "articles_transaction_color_aggr = transaction_article_colour.groupby(['t_dat','colour_group_name']).count().reset_index()[['t_dat','colour_group_name','article_id']]\n",
    "articles_transaction_color_aggr['t_dat'] = pd.to_datetime(articles_transaction_color_aggr['t_dat'])\n",
    "\n",
    "# Convert date into month and year\n",
    "articles_transaction_color_aggr['month_year']=articles_transaction_color_aggr['t_dat'].dt.strftime('%m/%Y')\n",
    "articles_transaction_color_aggr['year']= pd.DatetimeIndex(articles_transaction_color_aggr['t_dat']).year\n",
    "\n",
    "# Save csv\n",
    "articles_transaction_color_aggr.to_csv(\"data/articles_transaction_color_aggr.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the month of each date\n",
    "transactions['month'] =  pd.DatetimeIndex(transactions['t_dat']).month\n",
    "\n",
    "# Define the seasons along the year\n",
    "transactions.loc[(transactions[\"month\"] >= 3) & (transactions[\"month\"] <= 5) , \"season\"] = \"Spring\"\n",
    "transactions.loc[(transactions[\"month\"] >= 6) & (transactions[\"month\"] <= 8) , \"season\"] = \"Summer\"\n",
    "transactions.loc[(transactions[\"month\"] >= 9) & (transactions[\"month\"] <= 11) , \"season\"] = \"Autumn\"\n",
    "transactions.loc[(transactions[\"month\"] == 12) , \"season\"] = \"Winter\"\n",
    "transactions.loc[(transactions[\"month\"] >= 1) & (transactions[\"month\"] <= 2) , \"season\"] = \"Winter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Products per season\n",
    "transactions_season = transactions[['season' , 'article_id']].merge(articles[['article_id' , 'product_type_name' , 'product_type_no']] , on = 'article_id').groupby(['season' , 'product_type_no' ,'product_type_name']).agg({'product_type_no': 'count'}).rename(columns={'product_type_no': 'quantity'}).reset_index()\n",
    "\n",
    "# Save csv\n",
    "transactions_season.to_csv(\"data/transactions_season.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age Group\n",
    "import numpy as np\n",
    "bins = np.array([0,15,19,35,50,99])\n",
    "labels = ['unknown' , 'teens' , 'young' , 'middle-aged' , 'old']\n",
    "customers['age_cat'] = pd.cut(customers['age'], bins=bins, labels=labels, include_lowest=True)\n",
    "customers_age = customers[['customer_id' , 'age_cat']].merge(transactions[['customer_id' , 'season']] , on = 'customer_id').groupby(['season' , 'age_cat']).agg({'age_cat': 'count'}).rename(columns={'age_cat': 'quantity'}).reset_index()\n",
    "\n",
    "# Save csv\n",
    "customers_age.to_csv(\"data/customers_age.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors per season\n",
    "colors_season = transactions[['season' , 'article_id']].merge(articles[['article_id' , 'perceived_colour_master_name']] , on = 'article_id').groupby(['season' , 'perceived_colour_master_name']).agg({'perceived_colour_master_name': 'count'}).rename(columns={'perceived_colour_master_name': 'quantity'}).reset_index()\n",
    "# Save csv\n",
    "colors_season.to_csv(\"data/colors_season.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Product recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_MONTHS = 1\n",
    "\n",
    "window = relativedelta(months=N_MONTHS)\n",
    "last_date = transactions['t_dat'].max()\n",
    "\n",
    "threshold = last_date - window\n",
    "\n",
    "mask = transactions['t_dat'] > threshold\n",
    "transactions_baseline = transactions[mask]\n",
    "\n",
    "purchase_dict = {} # Dict that contains each article the user bought and the count of times it was bought\n",
    "\n",
    "for x in zip(transactions['customer_id'], transactions['article_id']):\n",
    "    cust_id, art_id = x\n",
    "    \n",
    "    if cust_id not in purchase_dict:\n",
    "        purchase_dict[cust_id] = {}\n",
    "    purchase_dict[cust_id][art_id] = purchase_dict[cust_id].get(art_id, 0) + 1\n",
    "    \n",
    "# List of the most bought articles for all users\n",
    "best_ever = list(transactions['article_id'].value_counts().index)\n",
    "\n",
    "# Save\n",
    "import pickle\n",
    "f = open(\"data/best_ever.pkl\", \"wb\")\n",
    "pickle.dump(best_ever, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/purchase_dict.pkl\", \"wb\")\n",
    "pickle.dump(purchase_dict, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(189510, 26252)\n",
      "Transactions from Sept 1, 2020:  727334\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "\n",
    "# Get the transactions from September 1, 2020 on\n",
    "transactions_copy = transactions.copy()\n",
    "transactions = transactions_copy[transactions_copy['t_dat'] > '2020-08-31'].sort_values(by=['customer_id'])\n",
    "\n",
    "# Count the number of same articles bought by the same person and convert to dataframe\n",
    "counts_df = transactions.groupby(['t_dat', 'customer_id', 'article_id', 'price', 'sales_channel_id']).size()\n",
    "counts_df = counts_df.to_frame()\n",
    "counts_df.reset_index(inplace=True)\n",
    "small_counts = counts_df.rename(columns={0: 'count'})\n",
    "\n",
    "# Transactions file after September 1, 2020 while the number of same articles bought by each person\n",
    "small_counts = small_counts.sort_values('customer_id')\n",
    "\n",
    "from scipy.sparse import csr_matrix, dok_matrix\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "# Auxiliary function\n",
    "def to_dense(array):\n",
    "    \"\"\"\n",
    "    Converts a spare matrix (where a lot of elements are zero) to a dense \n",
    "    array (an array where the elements are all sequential starting at index 0). \n",
    "    \n",
    "    :param array: Matrix to be converted\n",
    "    :return: Dense array\n",
    "    \"\"\"\n",
    "    try:\n",
    "        array = array.todense()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return np.array(array).squeeze()\n",
    "\n",
    "# Create sparse matrix user-item \n",
    "def build_counts_table(df):\n",
    "    \"\"\"\n",
    "    Gives an sparse matrix where the columns and the items and the rows the customer. \n",
    "    The value is the number of times that a customer has bought an item. \n",
    "    \n",
    "    :param df: original dataframe with transactions\n",
    "    :return: \n",
    "        * Sparse matrix\n",
    "        * Customer ids corresponding to each row\n",
    "        * Items ids corresponding to each column\n",
    "    \"\"\"\n",
    "    # Get customer ids and item ids\n",
    "    customer_ids = CategoricalDtype(sorted(df.customer_id.unique()), ordered=True)\n",
    "    item_ids = CategoricalDtype(sorted(df.article_id.unique()), ordered=True)\n",
    "\n",
    "    # Get sparse matrix\n",
    "    row = df.customer_id.astype(customer_ids).cat.codes\n",
    "    col = df.article_id.astype(item_ids).cat.codes\n",
    "    sparse_matrix = csr_matrix((df[\"count\"], (row, col)), \\\n",
    "                           shape=(customer_ids.categories.size, item_ids.categories.size))\n",
    "\n",
    "    return sparse_matrix, customer_ids, item_ids\n",
    "\n",
    "# Get sparse matrix for the transactions from Sept 1, 2020\n",
    "counts, indexes, columns = build_counts_table(small_counts)\n",
    "\n",
    "# Number of rows is number of customers, number of columns is number of articles\n",
    "print(counts.shape)\n",
    "\n",
    "# Get the ids of the top n customers\n",
    "def top_active_customers(counts, indexes, columns, n):\n",
    "    \"\"\"\n",
    "    Returns the id of the top n customers, in terms of items bought \n",
    "    \n",
    "    :param counts, indexes, columns: Tuple returned by build_counts_table\n",
    "    :param n: Number of users\n",
    "    :return: Series of customerID of the top users\n",
    "    \"\"\"\n",
    "    # Operate with the sparse matrix, convert to dense the result\n",
    "    sums = to_dense(counts.sum(axis=1))\n",
    "    # Get indices\n",
    "    indices = sums.argsort()\n",
    "    return indexes.categories[indices[-n:]]\n",
    "\n",
    "# Get the ids of the top n articles\n",
    "def top_bought_articles(counts, indexes, columns, n):\n",
    "    \"\"\"\n",
    "    Returns the top n most bought items\n",
    "    \n",
    "    :param counts, indexes, columns: Tuple returned by build_counts_table\n",
    "    :param n: Number of items\n",
    "    :return: Series of itemID of the top items\n",
    "    \"\"\"\n",
    "    # Operate with the sparse matrix, convert to dense the result\n",
    "    sums = to_dense(counts.sum(axis=0))\n",
    "    # Get indices\n",
    "    indices = sums.argsort()\n",
    "    return columns.categories[indices[-n:]]\n",
    "\n",
    "# Get the top 5,000 articles and users from the transactions after Sept 1, 2020\n",
    "top_customers = top_active_customers(counts, indexes, columns, 5000)\n",
    "top_items = top_bought_articles(counts, indexes, columns, 5000)\n",
    "\n",
    "# Transactions from Sept 1, 2020\n",
    "s = small_counts.copy()\n",
    "# Transactions from Sept 1, 2020 that include one of the most bought 5,000 items\n",
    "s = s[s.article_id.isin(top_items)]\n",
    "\n",
    "# Transactions from Sept 1, 2020 that include one of the most bought 5,000 items and belong to one of the most active 5,000 customers\n",
    "s = s[s.customer_id.isin(top_customers)]\n",
    "\n",
    "# Drop the non-relevant info\n",
    "s = s.drop(s.columns[[0, 3, 4]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sparse matrix for the transactions from Sept 1, 2020 that include the top n customers and items\n",
    "counts, indexes, columns = build_counts_table(s) # build the counts matrix\n",
    "\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# Compute similarities\n",
    "def similarity_matrix(similarity_function, counts): \n",
    "    # Convert the sparse matrix to a df\n",
    "    x = pd.DataFrame.sparse.from_spmatrix(counts)\n",
    "    # Create a pairwise distance matrix for the df considered using similarity function\n",
    "    matrix = pairwise_distances(X = x, metric = similarity_function, n_jobs = -1)\n",
    "    del x\n",
    "    # Get the sparse matrix of the pairwise distance matrix\n",
    "    matrix = csr_matrix(matrix)\n",
    "\n",
    "    return matrix\n",
    "\n",
    "# The similarity function to compute the predictions will be correlation\n",
    "similarities = similarity_matrix(similarity_function=\"correlation\", counts=counts.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4989x4903 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 69478 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CategoricalDtype(categories=['00077dbd5c4a4991e092e63893ccf29294a9d5c46e85010e95f2fc10bf9437a4',\n",
       "                  '000fb6e772c5d0023892065e659963da90b1866035558ec16fca51b0dcfb7e59',\n",
       "                  '004d932f7a27ac3167c77db81d9cfd89392729e7f7e0d4c27e57ba355fc93988',\n",
       "                  '004eba6e5f4705ea033b34f454b43524e41eb3d5c63923870c9845fb0c960706',\n",
       "                  '00754012108569f9c99871720111a2b50aa7b6ebebe2a415914df8b8e5e120ff',\n",
       "                  '007e3d72925742fd024933e33be4e5064815c6dc52c3fc98e23e505ef1edc828',\n",
       "                  '00902bb69f7c13348e4b781baa4316ce05755e1c5e327ace8177835b30e341fe',\n",
       "                  '0091bb09e49f45bdaeb3dd80f7bf98b4b8e3e3e49347e6bc9ec9818d631dddee',\n",
       "                  '00944aac87d67eb28bb5d3b5dc02dafa6b34c821ff6a3b788360da7e864703a5',\n",
       "                  '009a85913aa6f503ed0d2b5ac02ab919d6565bbbaa934a761ce376d643144f88',\n",
       "                  ...\n",
       "                  'ffbacd31aca5acf46e9afd95a633abb93b14ac3f1e40845efda1c49867497c3a',\n",
       "                  'ffbc3eed3d16c3e07668b5ef88a605fdae56d857e98972df5c05aa985719ae57',\n",
       "                  'ffbe3e5c5e3ef4484c2f193ea279ee9b2c994644781fe8fc9b0a75dea92860bb',\n",
       "                  'ffc197751ef56add717e78d7611d409b94c37d1f7aa900c88fb50cf3cdca90d5',\n",
       "                  'ffc247b933f175b37fccbb4f71c0479d6625e703b36f637be643afc224a8977f',\n",
       "                  'ffcd62aebc42dd8278eebf14775ab502f9d462d3b529669dd5bad0fe8bb80004',\n",
       "                  'ffd4cf2217de4a0a3f9f610cdec334c803692a18af08ac55b4c6fb955cc836b1',\n",
       "                  'ffe1791c9c6e3df9aafeebc77cf2cf03dd0123ac76ef90fb37c7b7c3fe6442e7',\n",
       "                  'fff2282977442e327b45d8c89afde25617d00124d0f99982410630ac51314356',\n",
       "                  'fff3573d9131d15da6a46c1ca8f03b5d37e4f6b804171ea27278a18a2d8ca0c7'],\n",
       "                 ordered=True)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CategoricalDtype(categories=[111565001, 111586001, 111593001, 111609001, 123173001,\n",
       "                  130035001, 146730001, 148033001, 153115019, 153115020,\n",
       "                  ...\n",
       "                  946795001, 946827001, 947060001, 947509001, 947934001,\n",
       "                  949198001, 949551001, 949551002, 952267001, 953763001],\n",
       "                 ordered=True)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "to_csv not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-10614d862edf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcsv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/indexes.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/columns.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: to_csv not found"
     ]
    }
   ],
   "source": [
    "# Save csv\n",
    "import io\n",
    "csv = io.StringIO()\n",
    "counts.to_csv(csv)\n",
    "indexes.to_csv(\"data/indexes.csv\",index=False)\n",
    "columns.to_csv(\"data/columns.csv\",index=False)\n",
    "similarities.to_csv(\"data/similarities.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
